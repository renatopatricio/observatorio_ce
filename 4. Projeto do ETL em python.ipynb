{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f1aae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "SERVER = 'DESKTOP-0NGHB9E'\n",
    "DATABASE = 'COMERCIO_DW'\n",
    "USERNAME = 'sa'\n",
    "PASSWORD = '123456'\n",
    "DRIVER = \"ODBC Driver 17 for SQL Server\"\n",
    "connection_string = f\"mssql+pyodbc://{USERNAME}:{PASSWORD}@{SERVER}/{DATABASE}?driver={DRIVER.replace(' ', '+')}\"\n",
    "engine = create_engine(connection_string)\n",
    "print(\" Conexão estabelecida com sucesso!\")\n",
    "tabelas = pd.read_sql(\"\"\"\n",
    "    SELECT TABLE_NAME\n",
    "    FROM INFORMATION_SCHEMA.TABLES\n",
    "    WHERE TABLE_TYPE = 'BASE TABLE'\n",
    "\"\"\", engine)\n",
    "print(\" Tabelas disponíveis no COMERCIO_DW:\")\n",
    "print(tabelas)\n",
    "tabelas_dim = [\n",
    "    \"fact.FATO_VENDAS\",\n",
    "    \"dim.DIM_CLIENTE\",\n",
    "    \"dim.DIM_PRODUTO\",\n",
    "    \"dim.DIM_TEMPO\",\n",
    "    \"dim.DIM_VENDEDOR\",\n",
    "    \"dim.DIM_FORNECEDOR\"\n",
    "]\n",
    "dfs = {}\n",
    "for tabela in tabelas_dim:\n",
    "    try:\n",
    "        df = pd.read_sql(f\"SELECT * FROM {tabela}\", engine)\n",
    "        dfs[tabela] = df\n",
    "        print(f\" {tabela} carregada com {len(df)} registros e {len(df.columns)} colunas\")\n",
    "    except Exception as e:\n",
    "        print(f\"️ Erro ao carregar {tabela}: {e}\")\n",
    "df_vendas = dfs.get(\"fact.FATO_VENDAS\")\n",
    "if df_vendas is not None:\n",
    "    display(df_vendas.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09dc71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "SERVER = 'DESKTOP-0NGHB9E'\n",
    "DATABASE = 'COMERCIO_DW'\n",
    "USERNAME = 'sa'\n",
    "PASSWORD = '123456'\n",
    "DRIVER = \"ODBC Driver 17 for SQL Server\"\n",
    "connection_string = f\"mssql+pyodbc://{USERNAME}:{PASSWORD}@{SERVER}/{DATABASE}?driver={DRIVER.replace(' ', '+')}\"\n",
    "engine = create_engine(connection_string)\n",
    "print(\" Conexão estabelecida com sucesso!\")\n",
    "query_pk = \"\"\"\n",
    "SELECT\n",
    "    tc.TABLE_SCHEMA,\n",
    "    tc.TABLE_NAME,\n",
    "    kcu.COLUMN_NAME\n",
    "FROM INFORMATION_SCHEMA.TABLE_CONSTRAINTS tc\n",
    "JOIN INFORMATION_SCHEMA.KEY_COLUMN_USAGE kcu\n",
    "    ON tc.CONSTRAINT_NAME = kcu.CONSTRAINT_NAME\n",
    "WHERE tc.CONSTRAINT_TYPE = 'PRIMARY KEY'\n",
    "ORDER BY tc.TABLE_SCHEMA, tc.TABLE_NAME;\n",
    "\"\"\"\n",
    "df_pk = pd.read_sql(query_pk, engine)\n",
    "print(\"\\n PRIMARY KEYS ENCONTRADAS:\")\n",
    "display(df_pk)\n",
    "query_fk = \"\"\"\n",
    "SELECT\n",
    "    fk.name AS FK_Name,\n",
    "    tp.name AS ParentTable,\n",
    "    ref.name AS ReferencedTable,\n",
    "    c1.name AS ParentColumn,\n",
    "    c2.name AS ReferencedColumn\n",
    "FROM sys.foreign_keys fk\n",
    "INNER JOIN sys.foreign_key_columns fkc\n",
    "    ON fkc.constraint_object_id = fk.object_id\n",
    "INNER JOIN sys.tables tp\n",
    "    ON fkc.parent_object_id = tp.object_id\n",
    "INNER JOIN sys.columns c1\n",
    "    ON fkc.parent_object_id = c1.object_id AND fkc.parent_column_id = c1.column_id\n",
    "INNER JOIN sys.tables ref\n",
    "    ON fkc.referenced_object_id = ref.object_id\n",
    "INNER JOIN sys.columns c2\n",
    "    ON fkc.referenced_object_id = c2.object_id AND fkc.referenced_column_id = c2.column_id\n",
    "ORDER BY tp.name, ref.name;\n",
    "\"\"\"\n",
    "df_fk = pd.read_sql(query_fk, engine)\n",
    "print(\"\\n FOREIGN KEYS ENCONTRADAS:\")\n",
    "display(df_fk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc4f5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "SERVER = 'DESKTOP-0NGHB9E'\n",
    "DATABASE = 'COMERCIO_DW'\n",
    "USERNAME = 'sa'\n",
    "PASSWORD = '123456'\n",
    "DRIVER = \"ODBC Driver 17 for SQL Server\"\n",
    "connection_string = f\"mssql+pyodbc://{USERNAME}:{PASSWORD}@{SERVER}/{DATABASE}?driver={DRIVER.replace(' ', '+')}\"\n",
    "engine = create_engine(connection_string)\n",
    "print(\" Conexão estabelecida com sucesso!\")\n",
    "campos_criticos = [\n",
    "    \"FATO_ID\",\n",
    "    \"DATA_SK\",\n",
    "    \"CLIENTE_SK\",\n",
    "    \"PRODUTO_SK\",\n",
    "    \"VENDEDOR_SK\",\n",
    "    \"QTDE\",\n",
    "    \"VALOR_UNITARIO\",\n",
    "    \"VL_TOTAL\"\n",
    "]\n",
    "resultados = []\n",
    "for campo in campos_criticos:\n",
    "    query = f\"SELECT COUNT(*) AS qtd_nulos FROM fact.FATO_VENDAS WHERE {campo} IS NULL\"\n",
    "    qtd_nulos = pd.read_sql(query, engine)['qtd_nulos'][0]\n",
    "    resultados.append({\"Campo\": campo, \"Nulos\": qtd_nulos})\n",
    "    if qtd_nulos > 0:\n",
    "        print(f\"️ {campo}: {qtd_nulos} valores nulos encontrados\")\n",
    "    else:\n",
    "        print(f\" {campo}: sem valores nulos\")\n",
    "df_nulos = pd.DataFrame(resultados)\n",
    "print(\"\\n Resumo final de nulos em fact.FATO_VENDAS:\")\n",
    "display(df_nulos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e391b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28740745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "SERVER = 'DESKTOP-0NGHB9E'\n",
    "DATABASE = 'COMERCIO_DW'\n",
    "USERNAME = 'sa'\n",
    "PASSWORD = '123456'\n",
    "DRIVER = \"ODBC Driver 17 for SQL Server\"\n",
    "connection_string = f\"mssql+pyodbc://{USERNAME}:{PASSWORD}@{SERVER}/{DATABASE}?driver={DRIVER.replace(' ', '+')}\"\n",
    "engine = create_engine(connection_string)\n",
    "print(\" Conexão estabelecida com sucesso!\")\n",
    "tipos_esperados = {\n",
    "    \"FATO_ID\": \"int\",\n",
    "    \"DATA_SK\": \"int\",\n",
    "    \"CLIENTE_SK\": \"int\",\n",
    "    \"PRODUTO_SK\": \"int\",\n",
    "    \"VENDEDOR_SK\": \"int\",\n",
    "    \"QTDE\": \"numeric\",\n",
    "    \"VALOR_UNITARIO\": \"numeric\",\n",
    "    \"VL_TOTAL\": \"numeric\"\n",
    "}\n",
    "print(\" Verificando tipos de dados na tabela fact.FATO_VENDAS...\\n\")\n",
    "query_tipos = \"\"\"\n",
    "SELECT\n",
    "    COLUMN_NAME,\n",
    "    DATA_TYPE,\n",
    "    CHARACTER_MAXIMUM_LENGTH,\n",
    "    NUMERIC_PRECISION,\n",
    "    NUMERIC_SCALE\n",
    "FROM INFORMATION_SCHEMA.COLUMNS\n",
    "WHERE TABLE_SCHEMA = 'fact'\n",
    "AND TABLE_NAME = 'FATO_VENDAS'\n",
    "ORDER BY ORDINAL_POSITION\n",
    "\"\"\"\n",
    "df_tipos_sql = pd.read_sql(query_tipos, engine)\n",
    "print(\" Tipos de dados definidos no SQL Server:\")\n",
    "print(df_tipos_sql)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "resultados_tipos = []\n",
    "for campo, tipo_esperado in tipos_esperados.items():\n",
    "    print(f\" Analisando campo: {campo}\")\n",
    "    if campo not in df_tipos_sql['COLUMN_NAME'].values:\n",
    "        print(f\" {campo}: Coluna não encontrada na tabela\")\n",
    "        resultados_tipos.append({\n",
    "            \"Campo\": campo,\n",
    "            \"Tipo_Esperado\": tipo_esperado,\n",
    "            \"Tipo_SQL\": \"N/A\",\n",
    "            \"Status\": \"COLUNA_NAO_ENCONTRADA\",\n",
    "            \"Inconsistencias\": 0\n",
    "        })\n",
    "        continue\n",
    "    tipo_sql = df_tipos_sql[df_tipos_sql['COLUMN_NAME'] == campo]['DATA_TYPE'].iloc[0]\n",
    "    inconsistencias = 0\n",
    "    status = \"OK\"\n",
    "    if tipo_esperado == \"int\":\n",
    "        query_int = f\"\"\"\n",
    "        SELECT COUNT(*) as inconsistencias\n",
    "        FROM fact.FATO_VENDAS\n",
    "        WHERE {campo} IS NOT NULL\n",
    "        AND (\n",
    "            ISNUMERIC(CAST({campo} AS VARCHAR)) = 0\n",
    "            OR CAST({campo} AS VARCHAR) LIKE '%.%'\n",
    "            OR CAST({campo} AS VARCHAR) LIKE '%,%'\n",
    "        )\n",
    "        \"\"\"\n",
    "        try:\n",
    "            inconsistencias = pd.read_sql(query_int, engine)['inconsistencias'][0]\n",
    "        except:\n",
    "            inconsistencias = 0\n",
    "    elif tipo_esperado == \"numeric\":\n",
    "        query_numeric = f\"\"\"\n",
    "        SELECT COUNT(*) as inconsistencias\n",
    "        FROM fact.FATO_VENDAS\n",
    "        WHERE {campo} IS NOT NULL\n",
    "        AND ISNUMERIC(CAST({campo} AS VARCHAR)) = 0\n",
    "        \"\"\"\n",
    "        try:\n",
    "            inconsistencias = pd.read_sql(query_numeric, engine)['inconsistencias'][0]\n",
    "        except:\n",
    "            inconsistencias = 0\n",
    "    elif tipo_esperado == \"date\":\n",
    "        query_date = f\"\"\"\n",
    "        SELECT COUNT(*) as inconsistencias\n",
    "        FROM fact.FATO_VENDAS\n",
    "        WHERE {campo} IS NOT NULL\n",
    "        AND ISDATE({campo}) = 0\n",
    "        \"\"\"\n",
    "        try:\n",
    "            inconsistencias = pd.read_sql(query_date, engine)['inconsistencias'][0]\n",
    "        except:\n",
    "            inconsistencias = 0\n",
    "    if inconsistencias > 0:\n",
    "        status = \"INCONSISTENTE\"\n",
    "        print(f\"️ {campo}: {inconsistencias} valores com tipo inconsistente\")\n",
    "    else:\n",
    "        print(f\" {campo}: tipos consistentes\")\n",
    "    resultados_tipos.append({\n",
    "        \"Campo\": campo,\n",
    "        \"Tipo_Esperado\": tipo_esperado,\n",
    "        \"Tipo_SQL\": tipo_sql,\n",
    "        \"Status\": status,\n",
    "        \"Inconsistencias\": inconsistencias\n",
    "    })\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\n Verificações adicionais de consistência...\\n\")\n",
    "query_data_sk = \"\"\"\n",
    "SELECT COUNT(*) as inconsistencias\n",
    "FROM fact.FATO_VENDAS\n",
    "WHERE DATA_SK IS NOT NULL\n",
    "AND (\n",
    "    LEN(CAST(DATA_SK AS VARCHAR)) != 8\n",
    "    OR DATA_SK < 19000101\n",
    "    OR DATA_SK > 21001231\n",
    "    OR ISDATE(\n",
    "        SUBSTRING(CAST(DATA_SK AS VARCHAR), 1, 4) + '-' +\n",
    "        SUBSTRING(CAST(DATA_SK AS VARCHAR), 5, 2) + '-' +\n",
    "        SUBSTRING(CAST(DATA_SK AS VARCHAR), 7, 2)\n",
    "    ) = 0\n",
    ")\n",
    "\"\"\"\n",
    "try:\n",
    "    data_sk_inconsistencias = pd.read_sql(query_data_sk, engine)['inconsistencias'][0]\n",
    "    if data_sk_inconsistencias > 0:\n",
    "        print(f\"️ DATA_SK: {data_sk_inconsistencias} valores com formato de data inválido\")\n",
    "    else:\n",
    "        print(\" DATA_SK: formato de data consistente\")\n",
    "except Exception as e:\n",
    "    print(f\" Erro ao verificar DATA_SK: {e}\")\n",
    "campos_positivos = [\"QTDE\", \"VALOR_UNITARIO\", \"VL_TOTAL\"]\n",
    "for campo in campos_positivos:\n",
    "    query_negativo = f\"\"\"\n",
    "    SELECT COUNT(*) as negativos\n",
    "    FROM fact.FATO_VENDAS\n",
    "    WHERE {campo} < 0\n",
    "    \"\"\"\n",
    "    try:\n",
    "        negativos = pd.read_sql(query_negativo, engine)['negativos'][0]\n",
    "        if negativos > 0:\n",
    "            print(f\"️ {campo}: {negativos} valores negativos encontrados\")\n",
    "        else:\n",
    "            print(f\" {campo}: sem valores negativos\")\n",
    "    except Exception as e:\n",
    "        print(f\" Erro ao verificar {campo}: {e}\")\n",
    "df_tipos = pd.DataFrame(resultados_tipos)\n",
    "print(\"\\n Resumo final de consistência de tipos em fact.FATO_VENDAS:\")\n",
    "print(df_tipos)\n",
    "df_problemas = df_tipos[df_tipos['Status'] != 'OK']\n",
    "if not df_problemas.empty:\n",
    "    print(\"\\n Campos com inconsistências de tipo:\")\n",
    "    print(df_problemas)\n",
    "else:\n",
    "    print(\"\\n Todos os campos estão com tipos consistentes!\")\n",
    "engine.dispose()\n",
    "print(\"\\n Conexão fechada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e598df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "SERVER = 'DESKTOP-0NGHB9E'\n",
    "DATABASE = 'COMERCIO_DW'\n",
    "USERNAME = 'sa'\n",
    "PASSWORD = '123456'\n",
    "DRIVER = \"ODBC Driver 17 for SQL Server\"\n",
    "connection_string = f\"mssql+pyodbc://{USERNAME}:{PASSWORD}@{SERVER}/{DATABASE}?driver={DRIVER.replace(' ', '+')}\"\n",
    "engine = create_engine(connection_string)\n",
    "print(\" Conexão estabelecida com sucesso!\")\n",
    "dimensoes_config = {\n",
    "    \"dim.DIM_CLIENTE\": {\n",
    "        \"chave_sk\": \"CLIENTE_SK\",\n",
    "        \"chave_fato\": \"CLIENTE_SK\",\n",
    "        \"nome_dimensao\": \"Cliente\"\n",
    "    },\n",
    "    \"dim.DIM_PRODUTO\": {\n",
    "        \"chave_sk\": \"PRODUTO_SK\",\n",
    "        \"chave_fato\": \"PRODUTO_SK\",\n",
    "        \"nome_dimensao\": \"Produto\"\n",
    "    },\n",
    "    \"dim.DIM_VENDEDOR\": {\n",
    "        \"chave_sk\": \"VENDEDOR_SK\",\n",
    "        \"chave_fato\": \"VENDEDOR_SK\",\n",
    "        \"nome_dimensao\": \"Vendedor\"\n",
    "    },\n",
    "    \"dim.DIM_TEMPO\": {\n",
    "        \"chave_sk\": \"DATA_SK\",\n",
    "        \"chave_fato\": \"DATA_SK\",\n",
    "        \"nome_dimensao\": \"Data\"\n",
    "    }\n",
    "}\n",
    "tabela_fato = \"fact.FATO_VENDAS\"\n",
    "print(\" Verificando registros órfãos nas dimensões...\\n\")\n",
    "resultados_orfaos = []\n",
    "for tabela_dim, config in dimensoes_config.items():\n",
    "    print(f\" Analisando dimensão: {config['nome_dimensao']} ({tabela_dim})\")\n",
    "    chave_sk = config['chave_sk']\n",
    "    chave_fato = config['chave_fato']\n",
    "    nome_dimensao = config['nome_dimensao']\n",
    "    try:\n",
    "        query_existe_dim = f\"\"\"\n",
    "        SELECT COUNT(*) as existe\n",
    "        FROM INFORMATION_SCHEMA.TABLES\n",
    "        WHERE TABLE_SCHEMA + '.' + TABLE_NAME = '{tabela_dim.replace('dim.', 'dim')}'\n",
    "        \"\"\"\n",
    "        existe_dim = pd.read_sql(query_existe_dim, engine)['existe'][0]\n",
    "        if existe_dim == 0:\n",
    "            print(f\" {nome_dimensao}: Tabela {tabela_dim} não encontrada\")\n",
    "            resultados_orfaos.append({\n",
    "                \"Dimensao\": nome_dimensao,\n",
    "                \"Tabela\": tabela_dim,\n",
    "                \"Total_Registros\": 0,\n",
    "                \"Registros_Orfaos\": 0,\n",
    "                \"Percentual_Orfaos\": 0,\n",
    "                \"Status\": \"TABELA_NAO_ENCONTRADA\"\n",
    "            })\n",
    "            continue\n",
    "        query_total = f\"SELECT COUNT(*) as total FROM {tabela_dim}\"\n",
    "        total_registros = pd.read_sql(query_total, engine)['total'][0]\n",
    "        query_orfaos = f\"\"\"\n",
    "        SELECT COUNT(*) as orfaos\n",
    "        FROM {tabela_dim} d\n",
    "        WHERE NOT EXISTS (\n",
    "            SELECT 1\n",
    "            FROM {tabela_fato} f\n",
    "            WHERE f.{chave_fato} = d.{chave_sk}\n",
    "        )\n",
    "        AND d.{chave_sk} IS NOT NULL\n",
    "        \"\"\"\n",
    "        registros_orfaos = pd.read_sql(query_orfaos, engine)['orfaos'][0]\n",
    "        percentual_orfaos = (registros_orfaos / total_registros * 100) if total_registros > 0 else 0\n",
    "        if registros_orfaos == 0:\n",
    "            status = \"OK\"\n",
    "            print(f\" {nome_dimensao}: Sem registros órfãos\")\n",
    "        elif percentual_orfaos <= 5:\n",
    "            status = \"ATENÇÃO\"\n",
    "            print(f\"️ {nome_dimensao}: {registros_orfaos} registros órfãos ({percentual_orfaos:.1f}%)\")\n",
    "        else:\n",
    "            status = \"CRÍTICO\"\n",
    "            print(f\" {nome_dimensao}: {registros_orfaos} registros órfãos ({percentual_orfaos:.1f}%)\")\n",
    "        resultados_orfaos.append({\n",
    "            \"Dimensao\": nome_dimensao,\n",
    "            \"Tabela\": tabela_dim,\n",
    "            \"Total_Registros\": total_registros,\n",
    "            \"Registros_Orfaos\": registros_orfaos,\n",
    "            \"Percentual_Orfaos\": round(percentual_orfaos, 2),\n",
    "            \"Status\": status\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\" Erro ao analisar {nome_dimensao}: {e}\")\n",
    "        resultados_orfaos.append({\n",
    "            \"Dimensao\": nome_dimensao,\n",
    "            \"Tabela\": tabela_dim,\n",
    "            \"Total_Registros\": 0,\n",
    "            \"Registros_Orfaos\": 0,\n",
    "            \"Percentual_Orfaos\": 0,\n",
    "            \"Status\": \"ERRO\"\n",
    "        })\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\n Verificando chaves na fato sem correspondência nas dimensões...\\n\")\n",
    "resultados_sem_dim = []\n",
    "for tabela_dim, config in dimensoes_config.items():\n",
    "    print(f\" Analisando chave: {config['chave_fato']} -> {config['nome_dimensao']}\")\n",
    "    chave_sk = config['chave_sk']\n",
    "    chave_fato = config['chave_fato']\n",
    "    nome_dimensao = config['nome_dimensao']\n",
    "    try:\n",
    "        query_sem_dim = f\"\"\"\n",
    "        SELECT COUNT(*) as sem_dimensao\n",
    "        FROM {tabela_fato} f\n",
    "        WHERE f.{chave_fato} IS NOT NULL\n",
    "        AND NOT EXISTS (\n",
    "            SELECT 1\n",
    "            FROM {tabela_dim} d\n",
    "            WHERE d.{chave_sk} = f.{chave_fato}\n",
    "        )\n",
    "        \"\"\"\n",
    "        sem_dimensao = pd.read_sql(query_sem_dim, engine)['sem_dimensao'][0]\n",
    "        query_total_fato = f\"\"\"\n",
    "        SELECT COUNT(*) as total\n",
    "        FROM {tabela_fato}\n",
    "        WHERE {chave_fato} IS NOT NULL\n",
    "        \"\"\"\n",
    "        total_fato = pd.read_sql(query_total_fato, engine)['total'][0]\n",
    "        percentual_sem_dim = (sem_dimensao / total_fato * 100) if total_fato > 0 else 0\n",
    "        if sem_dimensao == 0:\n",
    "            status = \"OK\"\n",
    "            print(f\" {nome_dimensao}: Todas as chaves da fato têm correspondência\")\n",
    "        elif percentual_sem_dim <= 1:\n",
    "            status = \"ATENÇÃO\"\n",
    "            print(f\"️ {nome_dimensao}: {sem_dimensao} registros na fato sem dimensão ({percentual_sem_dim:.1f}%)\")\n",
    "        else:\n",
    "            status = \"CRÍTICO\"\n",
    "            print(f\" {nome_dimensao}: {sem_dimensao} registros na fato sem dimensão ({percentual_sem_dim:.1f}%)\")\n",
    "        resultados_sem_dim.append({\n",
    "            \"Dimensao\": nome_dimensao,\n",
    "            \"Chave_Fato\": chave_fato,\n",
    "            \"Total_Fato\": total_fato,\n",
    "            \"Sem_Dimensao\": sem_dimensao,\n",
    "            \"Percentual_Sem_Dim\": round(percentual_sem_dim, 2),\n",
    "            \"Status\": status\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\" Erro ao analisar chave {chave_fato}: {e}\")\n",
    "        resultados_sem_dim.append({\n",
    "            \"Dimensao\": nome_dimensao,\n",
    "            \"Chave_Fato\": chave_fato,\n",
    "            \"Total_Fato\": 0,\n",
    "            \"Sem_Dimensao\": 0,\n",
    "            \"Percentual_Sem_Dim\": 0,\n",
    "            \"Status\": \"ERRO\"\n",
    "        })\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\n Detalhando registros órfãos (primeiros 10 de cada dimensão)...\\n\")\n",
    "for tabela_dim, config in dimensoes_config.items():\n",
    "    nome_dimensao = config['nome_dimensao']\n",
    "    chave_sk = config['chave_sk']\n",
    "    chave_fato = config['chave_fato']\n",
    "    query_detalhes = f\"\"\"\n",
    "    SELECT TOP 10 d.{chave_sk}, d.*\n",
    "    FROM {tabela_dim} d\n",
    "    WHERE NOT EXISTS (\n",
    "        SELECT 1\n",
    "        FROM {tabela_fato} f\n",
    "        WHERE f.{chave_fato} = d.{chave_sk}\n",
    "    )\n",
    "    AND d.{chave_sk} IS NOT NULL\n",
    "    ORDER BY d.{chave_sk}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df_detalhes = pd.read_sql(query_detalhes, engine)\n",
    "        if not df_detalhes.empty:\n",
    "            print(f\" Primeiros registros órfãos em {nome_dimensao}:\")\n",
    "            print(df_detalhes.head())\n",
    "            print()\n",
    "    except Exception as e:\n",
    "        print(f\" Erro ao detalhar {nome_dimensao}: {e}\")\n",
    "df_orfaos = pd.DataFrame(resultados_orfaos)\n",
    "df_sem_dim = pd.DataFrame(resultados_sem_dim)\n",
    "print(\"\\n Resumo final - Registros órfãos nas dimensões:\")\n",
    "print(df_orfaos)\n",
    "print(\"\\n Resumo final - Registros na fato sem dimensão:\")\n",
    "print(df_sem_dim)\n",
    "df_problemas_orfaos = df_orfaos[df_orfaos['Status'].isin(['CRÍTICO', 'ATENÇÃO'])]\n",
    "df_problemas_sem_dim = df_sem_dim[df_sem_dim['Status'].isin(['CRÍTICO', 'ATENÇÃO'])]\n",
    "if not df_problemas_orfaos.empty:\n",
    "    print(\"\\n Dimensões com registros órfãos:\")\n",
    "    print(df_problemas_orfaos[['Dimensao', 'Registros_Orfaos', 'Percentual_Orfaos', 'Status']])\n",
    "if not df_problemas_sem_dim.empty:\n",
    "    print(\"\\n Chaves na fato sem correspondência nas dimensões:\")\n",
    "    print(df_problemas_sem_dim[['Dimensao', 'Sem_Dimensao', 'Percentual_Sem_Dim', 'Status']])\n",
    "if df_problemas_orfaos.empty and df_problemas_sem_dim.empty:\n",
    "    print(\"\\n Todas as dimensões estão consistentes com a tabela fato!\")\n",
    "engine.dispose()\n",
    "print(\"\\n Conexão fechada.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8832506",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4cde2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df728152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932fc5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd62aa78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "SERVER = 'DESKTOP-0NGHB9E'\n",
    "DATABASE = 'COMERCIO_DW'\n",
    "USERNAME = 'sa'\n",
    "PASSWORD = '123456'\n",
    "DRIVER = \"ODBC Driver 17 for SQL Server\"\n",
    "connection_string = f\"mssql+pyodbc://{USERNAME}:{PASSWORD}@{SERVER}/{DATABASE}?driver={DRIVER.replace(' ', '+')}\"\n",
    "engine = create_engine(connection_string)\n",
    "print(\" Conexão estabelecida com sucesso!\")\n",
    "print(\"=\"*60)\n",
    "print(\" Extraindo dados da tabela fato...\")\n",
    "query_principal = \"\"\"\n",
    "SELECT\n",
    "    f.FATO_ID,\n",
    "    f.DATA_SK,\n",
    "    f.CLIENTE_SK,\n",
    "    f.PRODUTO_SK,\n",
    "    f.VENDEDOR_SK,\n",
    "    f.QTDE,\n",
    "    f.VALOR_UNITARIO,\n",
    "    f.VL_TOTAL,\n",
    "    f.COD_VENDA_ORIGEM,\n",
    "   -- Dados da dimensão cliente\n",
    "    c.COD_CLIENTE,\n",
    "    c.NOME            AS NOME_CLIENTE,\n",
    "    c.CPF,\n",
    "    c.EMAIL,\n",
    "    c.DT_NASCIMENTO,\n",
    "    c.ATIVO           AS CLIENTE_ATIVO,\n",
    "    -- Dados da dimensão produto\n",
    "    p.PRODUTO_SK,\n",
    "    p.COD_PRODUTO,\n",
    "    p.DESCRICAO,\n",
    "    p.CATEGORIA,\n",
    "    p.PRECO_ATUAL,\n",
    "    p.DT_INICIO,\n",
    "    p.DT_FIM,\n",
    "    p.ATIVO           AS PRODUTO_ATIVO,\n",
    "    p.HASH_ATTR,\n",
    "    -- Dados da dimensão vendedor\n",
    "    v.VENDEDOR_SK,\n",
    "    v.COD_VENDEDOR,\n",
    "    v.NOME           AS NOME_VENDEDOR,\n",
    "    v.REGIAO as REGIAO_VENDEDOR,\n",
    "    v.DT_ADMISSAO,\n",
    "    v.DT_INICIO,\n",
    "    v.DT_FIM,\n",
    "    v.ATIVO          AS VENDEDOR_ATIVO,\n",
    "    v.HASH_ATTR,\n",
    "    -- Dados da dimensão TEMPO\n",
    "    t.DATA_SK,\n",
    "    t.DATA_COMPLETA,\n",
    "    t.ANO,\n",
    "    t.MES,\n",
    "    t.TRIMESTRE,\n",
    "    t.NOME_MES,\n",
    "    t.DIA,\n",
    "    t.NOME_DIA,\n",
    "    t.FIM_DE_SEMANA\n",
    "FROM fact.FATO_VENDAS f\n",
    "LEFT JOIN dim.DIM_CLIENTE c ON f.CLIENTE_SK = c.CLIENTE_SK\n",
    "LEFT JOIN dim.DIM_PRODUTO p ON f.PRODUTO_SK = p.PRODUTO_SK\n",
    "LEFT JOIN dim.DIM_VENDEDOR v ON f.VENDEDOR_SK = v.VENDEDOR_SK\n",
    "LEFT JOIN dim.DIM_TEMPO t ON f.DATA_SK = t.DATA_SK\n",
    "WHERE f.VL_TOTAL IS NOT NULL\n",
    "\"\"\"\n",
    "df = pd.read_sql(query_principal, engine)\n",
    "df = df.loc[:, ~df.columns.duplicated()]\n",
    "print(f\" Dados extraídos: {len(df):,} registros\")\n",
    "print(f\" Colunas disponíveis: {len(df.columns)}\")\n",
    "print(\"\\n Primeiras linhas dos dados:\")\n",
    "print(df.head())\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" ANÁLISE EXPLORATÓRIA DOS DADOS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n Estatísticas descritivas das vendas:\")\n",
    "print(df[['QTDE', 'VALOR_UNITARIO', 'VL_TOTAL']].describe())\n",
    "print(f\"\\n Período dos dados: {df['ANO'].min()} a {df['ANO'].max()}\")\n",
    "print(f\" Categorias de produtos: {df['CATEGORIA'].nunique()}\")\n",
    "print(f\" Clientes únicos: {df['CLIENTE_SK'].nunique()}\")\n",
    "print(f\"‍ Vendedores únicos: {df['VENDEDOR_SK'].nunique()}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" CRIANDO COLUNAS DERIVADAS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n1️⃣ Criando faixas de venda baseadas em percentis...\")\n",
    "p25 = df['VL_TOTAL'].quantile(0.25)\n",
    "p75 = df['VL_TOTAL'].quantile(0.75)\n",
    "print(f\"    Percentil 25%: R$ {p25:,.2f}\")\n",
    "print(f\"    Percentil 75%: R$ {p75:,.2f}\")\n",
    "def classificar_venda(valor):\n",
    "    if valor <= p25:\n",
    "        return 'Low'\n",
    "    elif valor <= p75:\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'High'\n",
    "df['FAIXA_VENDA'] = df['VL_TOTAL'].apply(classificar_venda)\n",
    "print(\"    Coluna FAIXA_VENDA criada!\")\n",
    "print(f\"    Distribuição das faixas:\")\n",
    "print(df['FAIXA_VENDA'].value_counts())\n",
    "print(\"\\n2️⃣ Criando indicador de sazonalidade...\")\n",
    "def classificar_sazonalidade(mes):\n",
    "    if mes in [12, 1, 2]:\n",
    "        return 'Verão'\n",
    "    elif mes in [3, 4, 5]:\n",
    "        return 'Outono'\n",
    "    elif mes in [6, 7, 8]:\n",
    "        return 'Inverno'\n",
    "    else:\n",
    "        return 'Primavera'\n",
    "df['SAZONALIDADE'] = df['MES'].apply(classificar_sazonalidade)\n",
    "print(\"    Coluna SAZONALIDADE criada!\")\n",
    "print(f\"    Distribuição por estação:\")\n",
    "print(df['SAZONALIDADE'].value_counts())\n",
    "print(\"\\n3️⃣ Criando classificação de performance do vendedor...\")\n",
    "vendas_por_vendedor = df.groupby('VENDEDOR_SK')['VL_TOTAL'].sum().reset_index()\n",
    "vendas_por_vendedor.columns = ['VENDEDOR_SK', 'TOTAL_VENDAS_VENDEDOR']\n",
    "p33_vendedor = vendas_por_vendedor['TOTAL_VENDAS_VENDEDOR'].quantile(0.33)\n",
    "p66_vendedor = vendas_por_vendedor['TOTAL_VENDAS_VENDEDOR'].quantile(0.66)\n",
    "print(f\"    Percentil 33% vendedores: R$ {p33_vendedor:,.2f}\")\n",
    "print(f\"    Percentil 66% vendedores: R$ {p66_vendedor:,.2f}\")\n",
    "def classificar_performance_vendedor(total_vendas):\n",
    "    if total_vendas <= p33_vendedor:\n",
    "        return 'Baixa Performance'\n",
    "    elif total_vendas <= p66_vendedor:\n",
    "        return 'Média Performance'\n",
    "    else:\n",
    "        return 'Alta Performance'\n",
    "vendas_por_vendedor['PERFORMANCE_VENDEDOR'] = vendas_por_vendedor['TOTAL_VENDAS_VENDEDOR'].apply(classificar_performance_vendedor)\n",
    "df = df.merge(vendas_por_vendedor[['VENDEDOR_SK', 'TOTAL_VENDAS_VENDEDOR', 'PERFORMANCE_VENDEDOR']],\n",
    "              on='VENDEDOR_SK', how='left')\n",
    "print(\"    Coluna PERFORMANCE_VENDEDOR criada!\")\n",
    "print(f\"    Distribuição de performance:\")\n",
    "print(df['PERFORMANCE_VENDEDOR'].value_counts())\n",
    "print(\"\\n4️⃣ Criando classificação de ticket médio...\")\n",
    "df['TICKET_MEDIO'] = df['VL_TOTAL'] / df['QTDE']\n",
    "p50_ticket = df['TICKET_MEDIO'].quantile(0.50)\n",
    "def classificar_ticket(ticket):\n",
    "    if ticket <= p50_ticket:\n",
    "        return 'Ticket Baixo'\n",
    "    else:\n",
    "        return 'Ticket Alto'\n",
    "df['CLASSIFICACAO_TICKET'] = df['TICKET_MEDIO'].apply(classificar_ticket)\n",
    "print(\"    Colunas TICKET_MEDIO e CLASSIFICACAO_TICKET criadas!\")\n",
    "print(f\"    Ticket médio mediano: R$ {p50_ticket:.2f}\")\n",
    "print(f\"    Distribuição de tickets:\")\n",
    "print(df['CLASSIFICACAO_TICKET'].value_counts())\n",
    "print(\"\\n5️⃣ Criando categoria de cliente...\")\n",
    "vendas_por_cliente = df.groupby('CLIENTE_SK')['VL_TOTAL'].sum().reset_index()\n",
    "vendas_por_cliente.columns = ['CLIENTE_SK', 'TOTAL_VENDAS_CLIENTE']\n",
    "p20_cliente = vendas_por_cliente['TOTAL_VENDAS_CLIENTE'].quantile(0.20)\n",
    "p80_cliente = vendas_por_cliente['TOTAL_VENDAS_CLIENTE'].quantile(0.80)\n",
    "def classificar_cliente(total_vendas):\n",
    "    if total_vendas <= p20_cliente:\n",
    "        return 'Bronze'\n",
    "    elif total_vendas <= p80_cliente:\n",
    "        return 'Prata'\n",
    "    else:\n",
    "        return 'Ouro'\n",
    "vendas_por_cliente['CATEGORIA_CLIENTE'] = vendas_por_cliente['TOTAL_VENDAS_CLIENTE'].apply(classificar_cliente)\n",
    "df = df.merge(vendas_por_cliente[['CLIENTE_SK', 'TOTAL_VENDAS_CLIENTE', 'CATEGORIA_CLIENTE']],\n",
    "              on='CLIENTE_SK', how='left')\n",
    "print(\"    Coluna CATEGORIA_CLIENTE criada!\")\n",
    "print(f\"    Distribuição de categorias:\")\n",
    "print(df['CATEGORIA_CLIENTE'].value_counts())\n",
    "print(\"\\n6️⃣ Criando indicador de dia útil...\")\n",
    "def classificar_dia_util(dia_semana):\n",
    "    if dia_semana in [1, 7]:\n",
    "        return 'Fim de Semana'\n",
    "    else:\n",
    "        return 'Dia Útil'\n",
    "df['TIPO_DIA'] = df['FIM_DE_SEMANA'].apply(classificar_dia_util)\n",
    "print(\"    Coluna TIPO_DIA criada!\")\n",
    "print(f\"    Distribuição por tipo de dia:\")\n",
    "print(df['TIPO_DIA'].value_counts())\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" RESUMO DAS COLUNAS DERIVADAS CRIADAS\")\n",
    "print(\"=\"*60)\n",
    "colunas_derivadas = [\n",
    "    'FAIXA_VENDA', 'SAZONALIDADE', 'PERFORMANCE_VENDEDOR',\n",
    "    'TICKET_MEDIO', 'CLASSIFICACAO_TICKET', 'CATEGORIA_CLIENTE', 'TIPO_DIA'\n",
    "]\n",
    "print(f\" Total de colunas derivadas: {len(colunas_derivadas)}\")\n",
    "for i, coluna in enumerate(colunas_derivadas, 1):\n",
    "    print(f\"{i}. {coluna}\")\n",
    "print(f\"\\n Dataset final: {len(df)} registros x {len(df.columns)} colunas\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" ANÁLISES COM AS COLUNAS DERIVADAS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n1️⃣ Análise por Faixa de Venda:\")\n",
    "analise_faixa = df.groupby('FAIXA_VENDA').agg({\n",
    "    'VL_TOTAL': ['count', 'sum', 'mean'],\n",
    "    'QTDE': 'sum'\n",
    "}).round(2)\n",
    "print(analise_faixa)\n",
    "print(\"\\n2️⃣ Vendas por Sazonalidade:\")\n",
    "analise_sazon = df.groupby('SAZONALIDADE').agg({\n",
    "    'VL_TOTAL': ['count', 'sum', 'mean']\n",
    "}).round(2)\n",
    "print(analise_sazon)\n",
    "print(\"\\n3️⃣ Análise de Performance dos Vendedores:\")\n",
    "analise_vendedor = df.groupby('PERFORMANCE_VENDEDOR').agg({\n",
    "    'VL_TOTAL': ['count', 'sum', 'mean'],\n",
    "    'VENDEDOR_SK': 'nunique'\n",
    "}).round(2)\n",
    "print(analise_vendedor)\n",
    "print(\"\\n4️⃣ Análise por Categoria de Cliente:\")\n",
    "analise_cliente = df.groupby('CATEGORIA_CLIENTE').agg({\n",
    "    'VL_TOTAL': ['count', 'sum', 'mean'],\n",
    "    'CLIENTE_SK': 'nunique'\n",
    "}).round(2)\n",
    "print(analise_cliente)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" EXPORTANDO DADOS TRANSFORMADOS\")\n",
    "print(\"=\"*60)\n",
    "df.to_csv('dados_transformados_comercio_dw.csv', index=False, encoding='utf-8-sig')\n",
    "print(\" Dados exportados para 'dados_transformados_comercio_dw.csv'\")\n",
    "df_derivadas = df[['FATO_ID'] + colunas_derivadas]\n",
    "df_derivadas.to_csv('colunas_derivadas_comercio_dw.csv', index=False, encoding='utf-8-sig')\n",
    "print(\" Colunas derivadas exportadas para 'colunas_derivadas_comercio_dw.csv'\")\n",
    "print(f\"\\n RESUMO FINAL:\")\n",
    "print(f\"   • Registros processados: {len(df):,}\")\n",
    "print(f\"   • Colunas originais: {len(df.columns) - len(colunas_derivadas)}\")\n",
    "print(f\"   • Colunas derivadas criadas: {len(colunas_derivadas)}\")\n",
    "print(f\"   • Total de colunas: {len(df.columns)}\")\n",
    "engine.dispose()\n",
    "print(\"\\n Conexão com banco fechada.\")\n",
    "print(\" Processo de transformação concluído com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a25e232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5178084f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(\"\\n7️⃣ Criando coluna Novo vs Recorrente por cliente...\")\n",
    "df['DATA_COMPLETA'] = pd.to_datetime(df['DATA_COMPLETA'])\n",
    "primeira_compra_cliente = df.groupby('CLIENTE_SK')['DATA_COMPLETA'].min().reset_index()\n",
    "primeira_compra_cliente.columns = ['CLIENTE_SK', 'PRIMEIRA_COMPRA']\n",
    "df = df.merge(primeira_compra_cliente, on='CLIENTE_SK', how='left')\n",
    "df['NOVO_RECORRENTE'] = np.where(df['DATA_COMPLETA'] == df['PRIMEIRA_COMPRA'],\n",
    "                                 'Novo',\n",
    "                                 'Recorrente')\n",
    "print(\"    Coluna NOVO_RECORRENTE criada!\")\n",
    "print(f\"    Distribuição:\")\n",
    "print(df['NOVO_RECORRENTE'].value_counts())\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" CRIANDO DATAFRAMES PRÉ-AGRUPADOS\")\n",
    "print(\"=\"*60)\n",
    "df_vendas_mes_produto = (\n",
    "    df.groupby(['ANO', 'MES', 'COD_PRODUTO', 'DESCRICAO'])\n",
    "      .agg(\n",
    "          VENDAS_TOTAIS=('VL_TOTAL', 'sum'),\n",
    "          QTD_TOTAL=('QTDE', 'sum'),\n",
    "          TICKET_MEDIO=('VL_TOTAL', 'mean')\n",
    "      )\n",
    "      .reset_index()\n",
    "      .sort_values(['ANO', 'MES'])\n",
    ")\n",
    "print(\"\\n DataFrame 1: Vendas por Mês e Produto\")\n",
    "print(df_vendas_mes_produto.head())\n",
    "df_vendas_regiao_categoria = (\n",
    "    df.groupby(['REGIAO_VENDEDOR', 'CATEGORIA'])\n",
    "      .agg(\n",
    "          VENDAS_TOTAIS=('VL_TOTAL', 'sum'),\n",
    "          QTD_TOTAL=('QTDE', 'sum'),\n",
    "          PRODUTOS_DISTINTOS=('COD_PRODUTO', 'nunique'),\n",
    "          VENDEDORES_ATIVOS=('VENDEDOR_SK', 'nunique')\n",
    "      )\n",
    "      .reset_index()\n",
    "      .sort_values(['REGIAO_VENDEDOR', 'CATEGORIA'])\n",
    ")\n",
    "print(\"\\n DataFrame 2: Vendas por Região e Categoria de Produto\")\n",
    "print(df_vendas_regiao_categoria.head())\n",
    "print(\"\\n EXPORTANDO DATASET PRÉ-AGRUPADO...\")\n",
    "df_vendas_mes_produto.to_csv(\"vendas_por_mes_produto.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\" Arquivo 'vendas_por_mes_produto.csv' exportado com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
